#!/usr/bin/env python3
"""
æ±½è½¦å‡ºå£æ”¿ç­–æ–°é—»ç›‘æ§ (Auto Export Monitor)
==========================================
çˆ¬å–æƒå¨ç½‘ç«™æœ€æ–°æ”¿ç­–åŠ¨æ€ï¼ŒAIåˆ†æç”Ÿæˆé£é™©ç ”åˆ¤å’Œå†³ç­–å»ºè®®ã€‚
è¾“å‡ºæ ¼å¼åŒ–Markdownç®€æŠ¥ï¼Œä¾›OpenClawæ¨é€åˆ°é’‰é’‰ç¾¤ã€‚

Usage:
    python3 monitor.py              # å®Œæ•´è¿è¡Œï¼šçˆ¬å– + AIåˆ†æ
    python3 monitor.py --fetch-only # ä»…çˆ¬å–ï¼Œä¸è°ƒç”¨AIåˆ†æ
    python3 monitor.py --test       # æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬ç¬¬ä¸€ä¸ªæºçš„å‰3ç¯‡
"""

import os
import sys
import json
import time
import hashlib
import random
import argparse
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path

import requests
from bs4 import BeautifulSoup

# ============================================================
# é…ç½®å’Œåˆå§‹åŒ–
# ============================================================

SCRIPT_DIR = Path(__file__).parent
SOURCES_FILE = SCRIPT_DIR / "sources.json"
HISTORY_FILE = SCRIPT_DIR / "history.json"
OUTPUT_DIR = SCRIPT_DIR / "output"

# æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("auto-export-monitor")

# åŒ—äº¬æ—¶é—´
BJT = timezone(timedelta(hours=8))


def load_json(path: Path) -> dict:
    """åŠ è½½JSONæ–‡ä»¶"""
    if path.exists():
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_json(path: Path, data: dict):
    """ä¿å­˜JSONæ–‡ä»¶"""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def content_hash(text: str) -> str:
    """ç”Ÿæˆå†…å®¹å“ˆå¸Œï¼Œç”¨äºå¢é‡æ£€æµ‹"""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


# ============================================================
# çˆ¬è™«æ¨¡å—
# ============================================================

class NewsFetcher:
    """æ–°é—»çˆ¬å–å™¨ï¼šæ”¯æŒå¤šæºçˆ¬å–ã€å…³é”®è¯è¿‡æ»¤ã€å¢é‡æ£€æµ‹"""

    def __init__(self, config: dict, history: dict):
        self.sources = config["sources"]
        self.filter_keywords = config["filter_keywords"]
        self.settings = config["settings"]
        self.history = history
        self.session = requests.Session()

    def _get_headers(self) -> dict:
        """éšæœºUser-Agent"""
        ua = random.choice(self.settings["user_agents"])
        return {
            "User-Agent": ua,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

    def _delay(self):
        """éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°"""
        delay = random.uniform(
            self.settings["request_delay_min"],
            self.settings["request_delay_max"]
        )
        time.sleep(delay)

    def _fetch_page(self, url: str, encoding: str = "utf-8") -> str | None:
        """è·å–å•ä¸ªé¡µé¢HTML"""
        try:
            resp = self.session.get(
                url,
                headers=self._get_headers(),
                timeout=self.settings["request_timeout"],
                verify=False  # éƒ¨åˆ†æ”¿åºœç½‘ç«™SSLè¯ä¹¦æœ‰é—®é¢˜
            )
            resp.encoding = encoding
            if resp.status_code == 200:
                return resp.text
            else:
                log.warning(f"HTTP {resp.status_code}: {url}")
                return None
        except requests.RequestException as e:
            log.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None

    def _extract_articles_generic(self, html: str, base_url: str, source: dict) -> list[dict]:
        """
        é€šç”¨æ–‡ç« æå–å™¨
        ä»åˆ—è¡¨é¡µHTMLä¸­æå–æ–‡ç« æ ‡é¢˜å’Œé“¾æ¥ã€‚
        é‡‡ç”¨å¤šç§ç­–ç•¥åŒ¹é…ä¸åŒç½‘ç«™çš„é¡µé¢ç»“æ„ã€‚
        """
        soup = BeautifulSoup(html, "html.parser")
        articles = []
        seen_urls = set()

        # ç­–ç•¥1: æŸ¥æ‰¾å¸¸è§çš„æ–°é—»åˆ—è¡¨ç»“æ„
        # å¤§å¤šæ•°æ”¿åºœ/æœºæ„ç½‘ç«™ä½¿ç”¨ <li><a> æˆ– <div><a> ç»“æ„
        link_candidates = []

        # ä¼˜å…ˆæŸ¥æ‰¾æ–°é—»åˆ—è¡¨åŒºåŸŸå†…çš„é“¾æ¥
        for container_selector in [
            "div.news-list", "div.list", "ul.news_list", "div.content-list",
            "div.main-content", "div.article-list", "div.news",
            "div.list-content", "div.right-content", "div.con_list",
            "ul.list", "div.newsList", "div.news_con",
            "table.list", "div#list", "div.mod-list",
        ]:
            container = soup.select_one(container_selector)
            if container:
                link_candidates = container.find_all("a", href=True)
                if link_candidates:
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°å®¹å™¨ï¼Œå›é€€åˆ°å…¨é¡µé¢æœç´¢
        if not link_candidates:
            link_candidates = soup.find_all("a", href=True)

        for link in link_candidates:
            title = link.get_text(strip=True)
            href = link.get("href", "")

            # è¿‡æ»¤æ¡ä»¶
            if not title or len(title) < 6:
                continue
            if not href:
                continue

            # è·³è¿‡å¯¼èˆªé“¾æ¥ã€é”šç‚¹ã€JavaScript
            skip_patterns = [
                "javascript:", "#", "mailto:", "tel:",
                "index.html", "index.shtml", "index.htm",
                "/search", "/login", "/register",
            ]
            if any(p in href.lower() for p in skip_patterns):
                # ä½†ä¿ç•™åŒ…å«æ—¥æœŸæ¨¡å¼çš„indexé¡µé¢ï¼ˆæŸäº›ç½‘ç«™çš„æ–‡ç« é¡µå°±æ˜¯index.htmlï¼‰
                if not any(y in href for y in ["2024", "2025", "2026"]):
                    continue

            # æ„å»ºå®Œæ•´URL
            if href.startswith("http"):
                full_url = href
            elif href.startswith("//"):
                full_url = "https:" + href
            elif href.startswith("/"):
                from urllib.parse import urlparse
                parsed = urlparse(base_url)
                full_url = f"{parsed.scheme}://{parsed.netloc}{href}"
            else:
                full_url = base_url.rstrip("/") + "/" + href

            # å»é‡
            if full_url in seen_urls:
                continue
            seen_urls.add(full_url)

            articles.append({
                "title": title,
                "url": full_url,
                "source_id": source["id"],
                "source_name": source["name"],
                "category": source["category"],
                "fetch_time": datetime.now(BJT).isoformat(),
            })

        return articles[:self.settings["max_articles_per_source"]]

    def _is_relevant(self, article: dict, source: dict) -> bool:
        """
        å…³é”®è¯è¿‡æ»¤ï¼šåˆ¤æ–­æ–‡ç« æ˜¯å¦ä¸æ±½è½¦å‡ºå£ç›¸å…³ã€‚
        - auto_relevant=true çš„æºï¼šå¤©ç„¶ç›¸å…³ï¼ˆå¦‚æ±½è½¦æµé€šåä¼šï¼‰ï¼Œæ‰€æœ‰æ–‡ç« éƒ½ä¿ç•™
        - å…¶ä»–æºï¼šä½¿ç”¨ä¸¤çº§è¿‡æ»¤ï¼ˆæºä¸“å±å…³é”®è¯ + é€šç”¨å…³é”®è¯ï¼‰
        """
        # å¤©ç„¶ç›¸å…³çš„æºï¼Œæ‰€æœ‰æ–‡ç« éƒ½è§†ä¸ºç›¸å…³
        if source.get("auto_relevant", False):
            return True

        text = article["title"].lower()

        # æºä¸“å±å…³é”®è¯ï¼ˆä¼˜å…ˆçº§é«˜ï¼Œå‘½ä¸­ä¸€ä¸ªå³å¯ï¼‰
        for kw in source.get("keywords_boost", []):
            if kw.lower() in text:
                return True

        # é€šç”¨å…³é”®è¯ï¼ˆéœ€è¦å‘½ä¸­è‡³å°‘ä¸€ä¸ªï¼‰
        for kw in self.filter_keywords:
            if kw.lower() in text:
                return True

        return False

    def _is_new(self, article: dict) -> bool:
        """å¢é‡æ£€æµ‹ï¼šé€šè¿‡URLå“ˆå¸Œåˆ¤æ–­æ˜¯å¦å·²æŠ“å–è¿‡"""
        url_hash = content_hash(article["url"])
        return url_hash not in self.history.get("articles", {})

    def _mark_fetched(self, article: dict, detail_text: str = ""):
        """æ ‡è®°æ–‡ç« ä¸ºå·²æŠ“å–"""
        url_hash = content_hash(article["url"])
        if "articles" not in self.history:
            self.history["articles"] = {}
        self.history["articles"][url_hash] = {
            "title": article["title"],
            "url": article["url"],
            "source": article["source_name"],
            "fetched_at": datetime.now(BJT).isoformat(),
            "content_hash": content_hash(detail_text) if detail_text else "",
        }

    def _fetch_detail(self, url: str, encoding: str = "utf-8") -> str:
        """è·å–æ–‡ç« è¯¦æƒ…é¡µæ­£æ–‡"""
        html = self._fetch_page(url, encoding)
        if not html:
            return ""

        soup = BeautifulSoup(html, "html.parser")

        # ç§»é™¤scriptå’Œstyle
        for tag in soup(["script", "style", "nav", "header", "footer"]):
            tag.decompose()

        # å°è¯•å¤šç§æ­£æ–‡å®¹å™¨é€‰æ‹©å™¨
        content_selectors = [
            "div.article-content", "div.content", "div.TRS_Editor",
            "div.article", "div.detail-content", "div.news-content",
            "div.main-text", "div.text", "div.con_text", "div.artical",
            "article", "div#content", "div.pages_content",
            "div.Custom_UniformBlock", "div.article-body",
        ]

        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                text = content_div.get_text(separator="\n", strip=True)
                if len(text) > 100:  # æ­£æ–‡è‡³å°‘100å­—
                    return text[:5000]  # é™åˆ¶é•¿åº¦ï¼ŒèŠ‚çœAPIè°ƒç”¨æˆæœ¬

        # å›é€€ï¼šå–bodyå…¨éƒ¨æ–‡æœ¬
        body = soup.find("body")
        if body:
            text = body.get_text(separator="\n", strip=True)
            # å–ä¸­é—´éƒ¨åˆ†ï¼ˆè·³è¿‡å¤´å°¾å¯¼èˆªï¼‰
            lines = [l.strip() for l in text.split("\n") if len(l.strip()) > 15]
            return "\n".join(lines[:80])[:5000]

        return ""

    def fetch_all(self, test_mode: bool = False) -> list[dict]:
        """
        ä¸»çˆ¬å–æµç¨‹ï¼š
        1. é€æºçˆ¬å–åˆ—è¡¨é¡µ
        2. å…³é”®è¯è¿‡æ»¤
        3. å¢é‡æ£€æµ‹
        4. çˆ¬å–æ–°æ–‡ç« è¯¦æƒ…é¡µ
        """
        all_new_articles = []
        sources = self.sources[:1] if test_mode else self.sources

        for source in sources:
            log.info(f"ğŸ“¡ æ­£åœ¨çˆ¬å–: {source['name']} ({source['url']})")

            # çˆ¬å–åˆ—è¡¨é¡µ
            html = self._fetch_page(source["url"], source.get("encoding", "utf-8"))
            if not html:
                log.warning(f"âš ï¸  è·³è¿‡ {source['name']}ï¼šæ— æ³•è·å–é¡µé¢")
                continue

            # æå–æ–‡ç« åˆ—è¡¨
            articles = self._extract_articles_generic(html, source["url"], source)
            log.info(f"   æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")

            # å…³é”®è¯è¿‡æ»¤
            relevant = [a for a in articles if self._is_relevant(a, source)]
            log.info(f"   å…³é”®è¯åŒ¹é… {len(relevant)} ç¯‡")

            # å¢é‡æ£€æµ‹
            new_articles = [a for a in relevant if self._is_new(a)]
            log.info(f"   æ–°å¢æ–‡ç«  {len(new_articles)} ç¯‡")

            if test_mode:
                new_articles = new_articles[:3]

            # çˆ¬å–è¯¦æƒ…é¡µ
            detail_count = 0
            for article in new_articles:
                if detail_count >= self.settings["max_detail_fetch"]:
                    break

                log.info(f"   ğŸ“„ æŠ“å–è¯¦æƒ…: {article['title'][:40]}...")
                self._delay()
                detail_text = self._fetch_detail(article["url"], source.get("encoding", "utf-8"))

                if detail_text:
                    article["content"] = detail_text
                    detail_count += 1
                else:
                    article["content"] = article["title"]  # å›é€€åˆ°æ ‡é¢˜

                # æ ‡è®°ä¸ºå·²æŠ“å–
                self._mark_fetched(article, detail_text)
                all_new_articles.append(article)

            self._delay()

        # æ›´æ–°å†å²è®°å½•
        self.history["last_run"] = datetime.now(BJT).isoformat()
        save_json(HISTORY_FILE, self.history)

        log.info(f"\nâœ… çˆ¬å–å®Œæˆï¼šå…±è·å– {len(all_new_articles)} ç¯‡æ–°æ–‡ç« ")
        return all_new_articles


# ============================================================
# AIåˆ†ææ¨¡å—
# ============================================================

class PolicyAnalyzer:
    """æ”¿ç­–åˆ†æå™¨ï¼šè°ƒç”¨é€šä¹‰åƒé—®(qwen-max-latest)è¿›è¡ŒAIåˆ†æ"""

    def __init__(self):
        self.api_key = self._load_api_key()
        if not self.api_key:
            log.warning("âš ï¸  DASHSCOPE_API_KEY æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡AIåˆ†æ")

    def _load_api_key(self) -> str:
        """
        æŒ‰ä¼˜å…ˆçº§è¯»å– API Keyï¼š
        1. ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY
        2. ~/.openclaw/openclaw.json ä¸­çš„ dashscope é…ç½®
        """
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
        key = os.environ.get("DASHSCOPE_API_KEY", "")
        if key:
            log.info("ğŸ”‘ API Key æ¥æº: ç¯å¢ƒå˜é‡")
            return key

        # å›é€€ï¼šä» openclaw.json è¯»å–
        openclaw_config = Path.home() / ".openclaw" / "openclaw.json"
        if openclaw_config.exists():
            try:
                with open(openclaw_config, "r", encoding="utf-8") as f:
                    config = json.load(f)
                key = (config.get("models", {})
                       .get("providers", {})
                       .get("dashscope", {})
                       .get("apiKey", ""))
                if key:
                    log.info("ğŸ”‘ API Key æ¥æº: ~/.openclaw/openclaw.json")
                    return key
            except Exception as e:
                log.warning(f"è¯»å– openclaw.json å¤±è´¥: {e}")

        return ""

    def _call_qwen(self, prompt: str) -> str:
        """è°ƒç”¨é€šä¹‰åƒé—®APIï¼ˆå…¼å®¹OpenAIæ¥å£ï¼‰"""
        if not self.api_key:
            return ""

        try:
            from openai import OpenAI
            client = OpenAI(
                api_key=self.api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            response = client.chat.completions.create(
                model="qwen-max-latest",
                messages=[
                    {"role": "system", "content": self._system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
            )
            return response.choices[0].message.content
        except ImportError:
            log.warning("openai åº“æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ requests ç›´æ¥è°ƒç”¨")
            return self._call_qwen_raw(prompt)
        except Exception as e:
            log.error(f"AIåˆ†æå¤±è´¥: {e}")
            return ""

    def _call_qwen_raw(self, prompt: str) -> str:
        """å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ç”¨requestsè°ƒç”¨DashScope API"""
        try:
            resp = requests.post(
                "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "qwen-max-latest",
                    "messages": [
                        {"role": "system", "content": self._system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 2000,
                },
                timeout=60,
            )
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            log.error(f"AIåˆ†æå¤±è´¥(raw): {e}")
            return ""

    def _system_prompt(self) -> str:
        return """ä½ æ˜¯ã€Œé‘«æ™ºåœˆã€å¹³å°çš„æ±½è½¦äº§ä¸šå‡ºæµ·é£æ§åˆ†æå¸ˆã€‚
ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ¥è‡ªæƒå¨æœºæ„ï¼ˆå•†åŠ¡éƒ¨ã€æµ·å…³æ€»ç½²ã€ä¸­å›½ä¿¡ä¿ã€è¡Œä¸šåä¼šç­‰ï¼‰çš„æ”¿ç­–æ–‡ç« ï¼Œ
ä¸ºä¸­å›½æ±½è½¦åŠé›¶éƒ¨ä»¶å‡ºå£ä¼ä¸šï¼ˆä¸»æœºå‚ã€è´¸æ˜“å•†ï¼‰æä¾›ç²¾å‡†çš„é£é™©ç ”åˆ¤å’Œå†³ç­–å»ºè®®ã€‚

åˆ†æè¦æ±‚ï¼š
1. ç”¨"å¤§ç™½è¯"è¯´æ¸…æ”¿ç­–æ ¸å¿ƒï¼Œä¸è¦ç…§æ¬å®˜è¯
2. æ˜ç¡®é£é™©ç­‰çº§ï¼šğŸ”´é«˜å± / ğŸŸ¡å…³æ³¨ / ğŸŸ¢åˆ©å¥½
3. èšç„¦å¯¹æ±½è½¦å‡ºå£çš„å…·ä½“å½±å“
4. ç»™å‡ºå¯æ‰§è¡Œçš„åº”å¯¹å»ºè®®ï¼ˆä¸æ˜¯æ³›æ³›è€Œè°ˆï¼‰
5. å¦‚æ¶‰åŠé£é™©ï¼Œæé†’æ˜¯å¦éœ€è¦æŠ•ä¿å‡ºå£ä¿¡ç”¨é™©

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š
{
  "policy_brief": "ä¸€å¥è¯æ¦‚æ‹¬ï¼ˆä¸è¶…è¿‡25å­—ï¼‰",
  "risk_level": "red/yellow/green",
  "risk_type": "å…³ç¨å£å’/æŠ€æœ¯æ ‡å‡†/å¤–æ±‡ç®¡åˆ¶/æ”¿æ²»é£é™©/åå€¾é”€/äº§ä¸šæ‰¶æŒ/å¸‚åœºåŠ¨æ€/å…¶ä»–",
  "affected_segments": ["æ•´è½¦å‡ºå£", "é›¶éƒ¨ä»¶å‡ºå£", "å”®åå¸‚åœº", "æŠ•èµ„å»ºå‚"],
  "impact_analysis": "å¯¹ä¸»æœºå‚/é…ä»¶ç”Ÿäº§å•†/è´¸æ˜“å•†çš„å…·ä½“å½±å“ï¼ˆ2-3å¥è¯ï¼‰",
  "action_suggestions": [
    {"strategy": "ç­–ç•¥æ–¹å‘", "action": "å…·ä½“åŠ¨ä½œ"},
    {"strategy": "ç­–ç•¥æ–¹å‘", "action": "å…·ä½“åŠ¨ä½œ"}
  ],
  "insurance_hint": "æ˜¯å¦å»ºè®®æŠ•ä¿åŠç†ç”±ï¼ˆä¸€å¥è¯ï¼Œæ— éœ€æŠ•ä¿åˆ™å¡«ç©ºå­—ç¬¦ä¸²ï¼‰"
}"""

    def analyze_article(self, article: dict) -> dict:
        """åˆ†æå•ç¯‡æ–‡ç« """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ¥è‡ªã€Œ{article['source_name']}ã€çš„æ”¿ç­–/è¡Œä¸šæ–‡ç« ï¼š

æ ‡é¢˜ï¼š{article['title']}

æ­£æ–‡å†…å®¹ï¼š
{article.get('content', article['title'])[:3000]}

è¯·æŒ‰è¦æ±‚è¾“å‡ºJSONåˆ†æç»“æœã€‚"""

        log.info(f"ğŸ¤– AIåˆ†æ: {article['title'][:40]}...")
        result_text = self._call_qwen(prompt)

        if not result_text:
            return self._fallback_analysis(article)

        # è§£æJSON
        try:
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned = result_text.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.split("\n", 1)[1] if "\n" in cleaned else cleaned
                cleaned = cleaned.rsplit("```", 1)[0] if "```" in cleaned else cleaned
                cleaned = cleaned.strip()
            if cleaned.startswith("json"):
                cleaned = cleaned[4:].strip()

            analysis = json.loads(cleaned)
            return analysis
        except json.JSONDecodeError:
            log.warning(f"AIè¿”å›çš„JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
            return self._fallback_analysis(article, result_text)

    def _fallback_analysis(self, article: dict, raw_text: str = "") -> dict:
        """AIåˆ†æå¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆ"""
        return {
            "policy_brief": article["title"][:25],
            "risk_level": "yellow",
            "risk_type": "å…¶ä»–",
            "affected_segments": ["å¾…åˆ†æ"],
            "impact_analysis": raw_text[:200] if raw_text else "æ–‡ç« å†…å®¹å¾…äººå·¥åˆ†æã€‚",
            "action_suggestions": [
                {"strategy": "å…³æ³¨", "action": "å»ºè®®äººå·¥é˜…è¯»åŸæ–‡è¿›è¡Œåˆ¤æ–­"}
            ],
            "insurance_hint": ""
        }

    def analyze_batch(self, articles: list[dict]) -> list[dict]:
        """æ‰¹é‡åˆ†ææ–‡ç« """
        results = []
        for article in articles:
            analysis = self.analyze_article(article)
            article["ai_analysis"] = analysis
            results.append(article)
            time.sleep(1)  # APIè°ƒç”¨é—´éš”
        return results


# ============================================================
# è¾“å‡ºæ ¼å¼åŒ–æ¨¡å—
# ============================================================

class ReportFormatter:
    """æŠ¥å‘Šæ ¼å¼åŒ–å™¨ï¼šç”ŸæˆMarkdownæ ¼å¼çš„æ”¿ç­–æƒ…æŠ¥ç®€æŠ¥"""

    RISK_EMOJI = {
        "red": "ğŸ”´",
        "yellow": "ğŸŸ¡",
        "green": "ğŸŸ¢",
    }

    RISK_LABEL = {
        "red": "é«˜é£é™©",
        "yellow": "å…³æ³¨",
        "green": "åˆ©å¥½",
    }

    def format_report(self, articles: list[dict]) -> str:
        """ç”Ÿæˆå®Œæ•´çš„æ”¿ç­–æƒ…æŠ¥ç®€æŠ¥"""
        now = datetime.now(BJT)
        date_str = now.strftime("%Y-%m-%d")
        time_str = now.strftime("%H:%M")

        lines = []
        lines.append(f"ğŸ“‹ é‘«æ™ºåœˆÂ·æ”¿ç­–é£æ§å†…å‚")
        lines.append(f"ğŸ“… {date_str} {time_str}")
        lines.append(f"ğŸ“Š æœ¬æ¬¡ç›‘æ§åˆ° {len(articles)} æ¡æ–°åŠ¨æ€")
        lines.append("")

        if not articles:
            lines.append("âœ… ä»Šæ—¥æš‚æ— ä¸æ±½è½¦å‡ºå£ç›¸å…³çš„æ–°æ”¿ç­–åŠ¨æ€ã€‚")
            return "\n".join(lines)

        # æŒ‰é£é™©ç­‰çº§æ’åºï¼šred > yellow > green
        risk_order = {"red": 0, "yellow": 1, "green": 2}
        articles.sort(
            key=lambda a: risk_order.get(
                a.get("ai_analysis", {}).get("risk_level", "yellow"), 1
            )
        )

        for i, article in enumerate(articles, 1):
            analysis = article.get("ai_analysis", {})
            risk_level = analysis.get("risk_level", "yellow")
            emoji = self.RISK_EMOJI.get(risk_level, "ğŸŸ¡")
            label = self.RISK_LABEL.get(risk_level, "å…³æ³¨")

            lines.append("â”" * 30)
            lines.append(f"{emoji} [{label}] {analysis.get('policy_brief', article['title'][:25])}")
            lines.append(f"ğŸ“Œ æ¥æºï¼š{article['source_name']} | ç±»å‹ï¼š{analysis.get('risk_type', 'â€”')}")
            lines.append(f"ğŸ“ è§£è¯»ï¼š{analysis.get('impact_analysis', 'â€”')}")

            # å†³ç­–å»ºè®®
            suggestions = analysis.get("action_suggestions", [])
            if suggestions:
                lines.append("ğŸ’¡ å†³ç­–å»ºè®®ï¼š")
                for j, sug in enumerate(suggestions, 1):
                    lines.append(f"  {j}. [{sug.get('strategy', '')}] {sug.get('action', '')}")

            # ä¿¡ä¿æç¤º
            hint = analysis.get("insurance_hint", "")
            if hint:
                lines.append(f"ğŸ›¡ï¸ ä¿¡ä¿æç¤ºï¼š{hint}")

            lines.append(f"ğŸ”— åŸæ–‡ï¼š{article['url']}")
            lines.append("")

        lines.append("â”" * 30)
        lines.append("ğŸ“¡ æ•°æ®æºï¼šå•†åŠ¡éƒ¨/æµ·å…³æ€»ç½²/ä¸­å›½ä¿¡ä¿/æ±½è½¦æµé€šåä¼š")
        lines.append("ğŸ¤– åˆ†æå¼•æ“ï¼šé‘«æ™ºåœˆAIæ”¿ç­–åˆ†æç³»ç»Ÿ")
        lines.append("âš ï¸ ä»¥ä¸Šåˆ†æä»…ä¾›å‚è€ƒï¼Œå…·ä½“å†³ç­–è¯·ç»“åˆå®é™…æƒ…å†µ")

        return "\n".join(lines)

    def format_json_report(self, articles: list[dict]) -> dict:
        """ç”ŸæˆJSONæ ¼å¼çš„å®Œæ•´æŠ¥å‘Šï¼ˆä¾›åç»­ç³»ç»Ÿå¯¹æ¥ï¼‰"""
        now = datetime.now(BJT)
        return {
            "report_id": f"RPT{now.strftime('%Y%m%d%H%M')}",
            "generated_at": now.isoformat(),
            "report_type": "é‘«æ™ºåœˆÂ·æ”¿ç­–é£æ§å†…å‚",
            "total_articles": len(articles),
            "risk_summary": {
                "red": sum(1 for a in articles if a.get("ai_analysis", {}).get("risk_level") == "red"),
                "yellow": sum(1 for a in articles if a.get("ai_analysis", {}).get("risk_level") == "yellow"),
                "green": sum(1 for a in articles if a.get("ai_analysis", {}).get("risk_level") == "green"),
            },
            "articles": [
                {
                    "title": a["title"],
                    "url": a["url"],
                    "source": a["source_name"],
                    "category": a["category"],
                    "fetch_time": a["fetch_time"],
                    "ai_analysis": a.get("ai_analysis", {}),
                }
                for a in articles
            ],
        }


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="æ±½è½¦å‡ºå£æ”¿ç­–æ–°é—»ç›‘æ§")
    parser.add_argument("--fetch-only", action="store_true", help="ä»…çˆ¬å–ï¼Œä¸è°ƒç”¨AIåˆ†æ")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šä»…çˆ¬å–ç¬¬ä¸€ä¸ªæºçš„å‰3ç¯‡")
    args = parser.parse_args()

    # ç¦ç”¨SSLè­¦å‘Šï¼ˆéƒ¨åˆ†æ”¿åºœç½‘ç«™SSLè¯ä¹¦æœ‰é—®é¢˜ï¼‰
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    log.info("ğŸš— æ±½è½¦å‡ºå£æ”¿ç­–æ–°é—»ç›‘æ§ å¯åŠ¨")
    log.info(f"ğŸ“… {datetime.now(BJT).strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)")
    log.info("")

    # åŠ è½½é…ç½®
    config = load_json(SOURCES_FILE)
    if not config:
        log.error("âŒ sources.json é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")
        sys.exit(1)

    history = load_json(HISTORY_FILE)

    # Step 1: çˆ¬å–
    log.info("=" * 50)
    log.info("ğŸ“¡ Step 1: çˆ¬å–æ–°é—»åˆ—è¡¨")
    log.info("=" * 50)
    fetcher = NewsFetcher(config, history)
    new_articles = fetcher.fetch_all(test_mode=args.test)

    if not new_articles:
        log.info("ğŸ“­ æœ¬æ¬¡æ²¡æœ‰å‘ç°æ–°çš„ç›¸å…³æ–‡ç« ")
        # ä»ç„¶è¾“å‡ºç©ºæŠ¥å‘Š
        formatter = ReportFormatter()
        report_md = formatter.format_report([])
        print("\n" + report_md)
        return

    # Step 2: AIåˆ†æ
    if not args.fetch_only:
        log.info("")
        log.info("=" * 50)
        log.info("ğŸ¤– Step 2: AIæ”¿ç­–åˆ†æ")
        log.info("=" * 50)
        analyzer = PolicyAnalyzer()
        analyzed_articles = analyzer.analyze_batch(new_articles)
    else:
        analyzed_articles = new_articles
        log.info("â­ï¸  è·³è¿‡AIåˆ†æ (--fetch-only)")

    # Step 3: ç”ŸæˆæŠ¥å‘Š
    log.info("")
    log.info("=" * 50)
    log.info("ğŸ“‹ Step 3: ç”Ÿæˆæ”¿ç­–æƒ…æŠ¥ç®€æŠ¥")
    log.info("=" * 50)
    formatter = ReportFormatter()

    # Markdown æŠ¥å‘Šï¼ˆç”¨äºé’‰é’‰æ¨é€ï¼‰
    report_md = formatter.format_report(analyzed_articles)

    # JSON æŠ¥å‘Šï¼ˆç”¨äºåç»­ç³»ç»Ÿå¯¹æ¥ï¼‰
    report_json = formatter.format_json_report(analyzed_articles)

    # ä¿å­˜æŠ¥å‘Š
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    date_str = datetime.now(BJT).strftime("%Y%m%d_%H%M")

    md_path = OUTPUT_DIR / f"report_{date_str}.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(report_md)

    json_path = OUTPUT_DIR / f"report_{date_str}.json"
    save_json(json_path, report_json)

    log.info(f"ğŸ“„ MarkdownæŠ¥å‘Š: {md_path}")
    log.info(f"ğŸ“„ JSONæŠ¥å‘Š: {json_path}")

    # è¾“å‡ºåˆ°stdoutï¼ˆä¾›OpenClawè¯»å–å¹¶æ¨é€é’‰é’‰ï¼‰
    print("\n" + "=" * 50)
    print(report_md)
    print("=" * 50)

    log.info("\nâœ… ç›‘æ§å®Œæˆï¼")


if __name__ == "__main__":
    main()
