1
00:00:00,100 --> 00:00:03,120
最近香港大学开源了一个项目叫nana boat

2
00:00:03,120 --> 00:00:03,780
Nanobot

3
00:00:03,780 --> 00:00:06,090
是一款超轻量级个人AI助手

4
00:00:06,090 --> 00:00:09,450
仅用约4000行代码即可实现核心代理功能

5
00:00:09,450 --> 00:00:12,130
ano bot内置了五个skill cq

6
00:00:12,130 --> 00:00:14,290
本质上一个markdown文件里面

7
00:00:14,290 --> 00:00:16,129
用自然语言告诉LLM

8
00:00:16,129 --> 00:00:18,210
遇到某类任务时该怎么做

9
00:00:18,210 --> 00:00:19,730
它不是代码插件

10
00:00:19,730 --> 00:00:22,130
而是提示词工程的模块化封装

11
00:00:22,130 --> 00:00:24,510
把人类的最佳实践写成markdown

12
00:00:24,510 --> 00:00:26,560
在需要的时候喂给大模型

13
00:00:26,560 --> 00:00:28,060
这个设计很聪明

14
00:00:28,060 --> 00:00:30,900
你不需要会写代码就能给AI加能力

15
00:00:30,900 --> 00:00:34,210
我们让nano bot列出他的4Q和对应的功能

16
00:00:34,210 --> 00:00:38,250
skill create技能作用是让agent自己创建新技能

17
00:00:38,250 --> 00:00:39,890
它会自己生成skill点

18
00:00:39,890 --> 00:00:41,319
MD自我净化

19
00:00:41,319 --> 00:00:42,679
via天气查询

20
00:00:42,679 --> 00:00:44,479
用于获取天气和预报

21
00:00:44,479 --> 00:00:45,819
无需API密钥

22
00:00:45,819 --> 00:00:48,820
还有一些需要额外依赖的skill summarize

23
00:00:48,820 --> 00:00:51,820
内容摘要总结或提取文本转录内容

24
00:00:51,820 --> 00:00:53,720
team x终端会话管理

25
00:00:53,720 --> 00:00:55,860
用于远程控制TMAX会话

26
00:00:55,860 --> 00:00:57,540
GITHUB与GITHUB交互

27
00:00:57,540 --> 00:00:58,740
这里我们提问

28
00:00:58,740 --> 00:01:00,920
让他帮我们查询当地的天气

29
00:01:00,920 --> 00:01:04,200
LLM会调用RT file读取with a skill点

30
00:01:04,200 --> 00:01:04,660
Md

31
00:01:04,660 --> 00:01:06,990
按照skill指令执行返回结果

32
00:01:06,990 --> 00:01:08,510
我们看查询结果

33
00:01:08,510 --> 00:01:10,270
今日26年2月4日

34
00:01:10,270 --> 00:01:12,030
温度11.8℃

35
00:01:12,030 --> 00:01:13,290
天气多云阴天

36
00:01:13,290 --> 00:01:14,230
我跟他说

37
00:01:14,230 --> 00:01:16,020
帮我做一个翻译技能

38
00:01:18,480 --> 00:01:20,140
它会自己生成skill点

39
00:01:20,140 --> 00:01:22,020
MD写好触发条件

40
00:01:22,020 --> 00:01:23,600
放到workspace目录

41
00:01:23,600 --> 00:01:25,680
下次你说让他翻译这段话

42
00:01:25,680 --> 00:01:27,420
它就自动加载使用了

43
00:01:27,420 --> 00:01:29,600
下面我们来一起本地部署NANO

44
00:01:29,600 --> 00:01:31,560
but他用了light lamb做模型

45
00:01:31,560 --> 00:01:34,120
路由理论上支持100多个模型

46
00:01:34,120 --> 00:01:37,990
OpenAI智谱DPC gamma本地模型

47
00:01:37,990 --> 00:01:41,230
我们按照官方命令克隆项目并安装依赖

48
00:01:41,230 --> 00:01:44,090
安装依赖完成后运行初始化命令

49
00:01:44,090 --> 00:01:46,430
它会生成CONFIG点JASON文件

50
00:01:46,430 --> 00:01:48,890
这里我们用的是性价比高的质朴

51
00:01:48,890 --> 00:01:50,700
GLM4.7模型

52
00:01:50,700 --> 00:01:53,280
需要注意的是前缀过时问题

53
00:01:53,280 --> 00:01:56,240
nano bot代码里用的前缀是吉普

54
00:01:56,240 --> 00:02:00,210
但light tella米津在25年12月改成了ZI质谱

55
00:02:00,210 --> 00:02:02,690
不认识老前缀处理后的APIT了

56
00:02:02,690 --> 00:02:04,789
所以爆令牌已过期错误

57
00:02:04,789 --> 00:02:07,829
我们需要修改schema点PY和lion provider

58
00:02:07,829 --> 00:02:08,749
点PY文件

59
00:02:08,749 --> 00:02:10,309
原来字段叫GPU

60
00:02:10,309 --> 00:02:11,620
改成ZAI后

61
00:02:11,620 --> 00:02:13,620
nanobot才能正确读到你

62
00:02:13,620 --> 00:02:17,380
CONFIG点JASON里ZI下面的API key深度对比

63
00:02:17,380 --> 00:02:19,270
open class和nanobot架构

64
00:02:19,270 --> 00:02:21,630
下面是我觉得最值得聊的部分

65
00:02:21,630 --> 00:02:22,770
token经济学

66
00:02:22,770 --> 00:02:24,330
用AI agent做事

67
00:02:24,330 --> 00:02:26,750
每一次对话都要把系统提示词

68
00:02:26,750 --> 00:02:27,790
工具定义

69
00:02:27,790 --> 00:02:29,970
技能描述打包发给大模型

70
00:02:29,970 --> 00:02:31,890
这些东西每次都要花钱

71
00:02:31,890 --> 00:02:33,230
哪怕你只是问一句

72
00:02:33,230 --> 00:02:34,540
2+2等于几

73
00:02:34,540 --> 00:02:38,190
open cloud系统提示词大概1万4000个token

74
00:02:38,190 --> 00:02:39,990
工具定义8000个token

75
00:02:39,990 --> 00:02:41,330
你还没问问题呢

76
00:02:41,330 --> 00:02:43,820
光底噪就已经22000token了

77
00:02:43,820 --> 00:02:46,740
nano bot系统提示词压缩到3000~5000

78
00:02:46,740 --> 00:02:49,820
token工具只保留了九个核心的定义

79
00:02:49,820 --> 00:02:51,460
开销2500token

80
00:02:51,460 --> 00:02:54,760
同样的简单查询大概6600token

81
00:02:54,760 --> 00:02:56,320
省了72%

82
00:02:56,320 --> 00:02:57,720
再看复杂任务

83
00:02:57,720 --> 00:02:59,910
比如读文件分析内容

84
00:02:59,910 --> 00:03:01,910
搜索网页跑五轮工具

85
00:03:01,910 --> 00:03:05,070
调用open cloud要烧掉120000token的输入

86
00:03:05,070 --> 00:03:06,970
nano bot只要3.5万

87
00:03:06,970 --> 00:03:08,140
差了三倍多

88
00:03:08,140 --> 00:03:09,760
为什么差距这么大

89
00:03:09,760 --> 00:03:11,540
我拆了七个维度来看

90
00:03:11,540 --> 00:03:13,560
第一个系统提示词

91
00:03:13,560 --> 00:03:15,370
open clash是全量注入

92
00:03:15,370 --> 00:03:16,630
不管你问什么

93
00:03:16,630 --> 00:03:18,649
底噪固定14000token

94
00:03:18,649 --> 00:03:20,969
nano bot做了精简核心指令

95
00:03:20,969 --> 00:03:23,869
压缩技能只放一个XML索引摘要

96
00:03:23,869 --> 00:03:25,780
第二个反射循环

97
00:03:25,780 --> 00:03:27,500
这个差异非常关键

98
00:03:27,500 --> 00:03:29,820
open cloud有深度reflection机制

99
00:03:29,820 --> 00:03:31,200
出错了会反思

100
00:03:31,200 --> 00:03:32,500
反思完重试

101
00:03:32,500 --> 00:03:34,120
重试失败再反思

102
00:03:34,120 --> 00:03:35,480
听起来很智能

103
00:03:35,480 --> 00:03:36,780
但最坏情况下

104
00:03:36,780 --> 00:03:39,280
五轮反射成本直接放大十倍

105
00:03:39,280 --> 00:03:42,010
nano bot没有反射失败就报错

106
00:03:42,010 --> 00:03:44,110
好处是绝对不会成本失控

107
00:03:44,110 --> 00:03:46,800
坏处是复杂任务的自愈能力弱一些

108
00:03:46,800 --> 00:03:49,040
第三个工具加载策略

109
00:03:49,040 --> 00:03:51,740
open cloud是20多个工具全量注入

110
00:03:51,740 --> 00:03:53,640
加上所有skill原数据

111
00:03:53,640 --> 00:03:56,140
假设你十次对话只用了一个skill

112
00:03:56,140 --> 00:03:57,820
剩下九次全是浪费

113
00:03:57,820 --> 00:03:59,140
会白白浪费10万

114
00:03:59,140 --> 00:04:03,289
token nanobot用了一个巧妙的两级加载系统

115
00:04:03,289 --> 00:04:06,489
提示词里只放技能的XML索引大模型

116
00:04:06,489 --> 00:04:09,789
看到索引后自己决定要不要加载完整内容

117
00:04:09,789 --> 00:04:11,930
第四个上下文压缩

118
00:04:11,930 --> 00:04:13,850
open cloud有auto compaction

119
00:04:13,850 --> 00:04:15,110
但预计设在5万

120
00:04:15,110 --> 00:04:18,209
token nanobot完全没有压缩消息

121
00:04:18,209 --> 00:04:19,769
只会追加敲线了

122
00:04:19,769 --> 00:04:21,430
API直接报400错误

123
00:04:21,430 --> 00:04:23,700
第五个工具返回结果

124
00:04:23,700 --> 00:04:26,900
open cloud浏览器工具可能返回完整网页

125
00:04:26,900 --> 00:04:30,020
HTML动辄几万token nanobot

126
00:04:30,020 --> 00:04:32,860
用brave search API返回结构化摘要

127
00:04:32,860 --> 00:04:35,850
不过shell和文件读取两边都没做截断

128
00:04:35,850 --> 00:04:38,930
但是这个brave search API申请需要绑卡

129
00:04:38,930 --> 00:04:41,110
大家有条件的可以尝试一下

130
00:04:41,110 --> 00:04:42,490
第六个session

131
00:04:42,490 --> 00:04:43,330
历史管理

132
00:04:43,330 --> 00:04:45,190
open cloud保存完整消息

133
00:04:45,190 --> 00:04:48,310
包括中间的工具调用列50轮对话后

134
00:04:48,310 --> 00:04:51,610
历史可能超过50000token novo two只存问答

135
00:04:51,610 --> 00:04:53,990
对中间的工具调用过程不存

136
00:04:53,990 --> 00:04:55,550
所以增长慢很多

137
00:04:55,550 --> 00:04:57,990
50轮也就一点50000token左右

138
00:04:57,990 --> 00:04:59,500
再看skills系统

139
00:05:02,520 --> 00:05:04,419
但加载策略完全不同

140
00:05:04,419 --> 00:05:06,179
open cloud是全量注入

141
00:05:06,179 --> 00:05:08,580
所有技能都塞进系统提示词

142
00:05:08,580 --> 00:05:11,080
好处是模型随时能用任何工具

143
00:05:11,080 --> 00:05:12,220
坏处是不管用

144
00:05:12,220 --> 00:05:13,630
不用都占token

145
00:05:13,630 --> 00:05:17,470
nano bot在系统提示词里只放一个技能索引列表

146
00:05:17,470 --> 00:05:20,220
模型看到索引后判断需要哪个技能

147
00:05:20,220 --> 00:05:22,660
再用read file工具加载完整内容

148
00:05:22,660 --> 00:05:23,940
多花一轮调用

149
00:05:23,940 --> 00:05:25,340
但省了大量token

150
00:05:25,340 --> 00:05:29,350
你看现在cloud的system prompt里也有类似的seals列表

151
00:05:29,350 --> 00:05:32,230
MCP协议本质上也在做同样的事

152
00:05:32,230 --> 00:05:34,690
skills这个模式以后一定是标配

153
00:05:34,690 --> 00:05:36,830
不管什么AI agent框架

154
00:05:36,830 --> 00:05:38,190
最终都要解决

155
00:05:38,190 --> 00:05:41,500
怎么把领域知识高效注入给模型这个问题

156
00:05:41,500 --> 00:05:42,880
最后总结一下

157
00:05:42,880 --> 00:05:44,500
open clothes定位全能

158
00:05:44,500 --> 00:05:45,100
但昂贵

159
00:05:45,100 --> 00:05:47,230
它的功能覆盖确实最全

160
00:05:47,230 --> 00:05:50,570
反射机制让他处理复杂任务时有自愈能力

161
00:05:50,570 --> 00:05:53,130
但代价是每次对话的固定成本高

162
00:05:53,130 --> 00:05:54,870
存在成本失控风险

163
00:05:54,870 --> 00:05:57,190
nano bot代码量只有1%

164
00:05:57,190 --> 00:05:59,390
但该有的react循环工具

165
00:05:59,390 --> 00:06:01,530
调用技能系统一样没少

166
00:06:01,530 --> 00:06:03,190
它不适合所有场景

167
00:06:03,190 --> 00:06:05,900
但对于个人开发者和小团队来说

168
00:06:05,900 --> 00:06:07,060
性价比极高

169
00:06:07,060 --> 00:06:08,360
这期就到这里

170
00:06:08,360 --> 00:06:09,460
觉得有用的话

171
00:06:09,460 --> 00:06:09,940
点个赞

172
00:06:09,940 --> 00:06:10,760
收藏一下

173
00:06:10,760 --> 00:06:12,380
我们下期视频再见

